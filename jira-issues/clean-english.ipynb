{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jan  8 18:33:01 2019\n",
    "\n",
    "@author: sameamin\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "#from num2words import num2words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_read_file(file = None, column = None):\n",
    "    #df = pd.read_excel(file, encoding = 'utf-8')\n",
    "    df = pd.read_csv(file, encoding = 'utf-8')\n",
    "    return df\n",
    "\n",
    "\n",
    "def pipeline_remove_duplicates_empty(tokens):\n",
    "    tokens = list(filter(None, tokens))\n",
    "    #tokens = [ token.strip() for  token in tokens if len(token.strip()) > 0]\n",
    "    tokens = list(set(tokens))\n",
    "    return tokens\n",
    "\n",
    "def pipeline_remove_specials_spaces(tokens):\n",
    "    tokens = [ re.sub(\"[^A-Za-z0-9']+\", ' ', str(token)) for  token in tokens]\n",
    "    #tokens = [token.strip() for  token in tokens if len(token.strip()) > 1]\n",
    "    return tokens\n",
    "\n",
    "def pipeline_remove_abbrev(tokens):\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        token = tokens[i]\n",
    "        \n",
    "        # remove single chars\n",
    "        token = ' '.join([w for w in str(token).split() if len(w) > 1])\n",
    "        \n",
    "        # remove wrong abbreviations\n",
    "        abbrevs_wrong = ['gen', 'gen\\'s', 'llc', 'co', 'co.', 'tr', 'fze', 'est', 'nt', 'br', 'fzco',\n",
    "                   'ltd', 'dmcc', 'fzc', 'fz', 'cont', 'mohd', 'ind',\n",
    "                   'sh', 'dwc', 'limited', 'eng', 'rep', 'inc', 'difc', 'dsg', 'contg', 'lc',\n",
    "                   'trd', 'int', 'emb', 'const', 'trd', 'll', 'px', 'adv', 'pvt', 'ind', \n",
    "                   'gmbh', 'con', 'ap', 'mea', 'llp', 'dx', 'pte', 'ag', 'sa', 'md', 'mrw',\n",
    "                   'corp', 'pub', 'fzd', 'psc', 'es', 'contr', 'estb', 'eqpt', 'bu', 'hh',\n",
    "                   'dsoa', 'ghq', 'lcc', 'ent', 'exhb', 'serv', 'ink', 'dist', 'ab', 'kg',\n",
    "                   'hq', 'cons', 'bv', 'tra', 'wll', 'svc', 'nd', 'ad', 'lle', 'caf', 'comp',\n",
    "                   'sal', 'slc', 'pdxb', 'trdg', 'trdgco', 'cowll', 'srl', 'coltd',\n",
    "                   'zllcf', 'mfg', 'jv', 'pjsc', 'wwl', 'ser', 'pmdc', 'lda',\n",
    "                   'dept', 'trllc', 'fzllc', 'collc', 'foodstuff', 'catering', 'indllc',\n",
    "                   'trdcoltd', 'trdgest', 'fzoc', 'ltdco', 'lllc', 'col', 'tradcollc', \n",
    "                   'corpn', 'trdllc', 'trdest', 'indltd', 'llcc', 'equiptrest', 'contcollc', \n",
    "                   'servicesllc', 'llcbr', 'ltdd', 'contllc', 'eastfze', 'llco', 'lll',\n",
    "                   'icd', 'tradg', 'fzer', 'sgj', 'llcons', 'bldconco', 'jlt',\n",
    "                   'jafza', 'tradin']\n",
    "        token = ' '.join([w for w in str(token).split() if w.lower() not in abbrevs_wrong])\n",
    "\n",
    "        # capitalize token\n",
    "        # token = ' '.join( [word.capitalize() for word in token.lower().split()] )\n",
    "    return tokens\n",
    "\n",
    "#def pipeline_convert_numbers(tokens):\n",
    "#    # TODO: convert numbers\n",
    "#    for i in range(len(tokens)):\n",
    "#        \n",
    "#        tmp_token = ''\n",
    "#        for w in tokens[i].split():\n",
    "#            if w.isnumeric():\n",
    "#                tmp_token += (num2words(int(w)) + ' ')\n",
    "#            else:\n",
    "#                tmp_token += (w + ' ')\n",
    "#        tokens[i] = tmp_token.strip()\n",
    "#\n",
    "#    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====================================================================    \n",
    "# pipeline Start\n",
    "#==============Tarek-26_3-101-800=Watson-Utterances-Missing-EN.xlsx=============\n",
    "source_file_name = 'data/3-jira-101-en.csv'\n",
    "\n",
    "# read the file\n",
    "original_data = pipeline_read_file(source_file_name)\n",
    "\n",
    "tokens = original_data['Utterances'].values\n",
    "\n",
    "# convert numbers to text\n",
    "#tokens = pipeline_convert_numbers(tokens)\n",
    "\n",
    "# special charachters and spaces\n",
    "tokens = pipeline_remove_specials_spaces(tokens)\n",
    "\n",
    "# separate Al arabic char\n",
    "#tokens = separate_al_char(tokens)\n",
    "\n",
    "# should be the last step - remove abbreviations \n",
    "tokens = pipeline_remove_abbrev(tokens)\n",
    "\n",
    "# remove duplicates\n",
    "tokens = pipeline_remove_duplicates_empty(tokens)\n",
    "\n",
    "data = pd.DataFrame({'Utterances': tokens})\n",
    "data.head()\n",
    "data.describe()\n",
    "\n",
    "# check term frequently \n",
    "#df_term_freq = get_term_frequently(data)\n",
    "\n",
    "# write the file\n",
    "data.to_csv(\"data/4-jira-101-en-cleaned.csv\", encoding='utf-8', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
